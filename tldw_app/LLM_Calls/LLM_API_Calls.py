# LLM_API_Calls.py
#########################################
# General LLM API Calling Library
# This library is used to perform API Calls against commercial LLM endpoints.
#
####
####################
# Function List
#
# 1. extract_text_from_segments(segments: List[Dict]) -> str
# 2. chat_with_openai(api_key, file_path, custom_prompt_arg, streaming=None)
# 3. chat_with_anthropic(api_key, file_path, model, custom_prompt_arg, max_retries=3, retry_delay=5, streaming=None)
# 4. chat_with_cohere(api_key, file_path, model, custom_prompt_arg, streaming=None)
# 5. chat_with_groq(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None):
# 6. chat_with_openrouter(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 7. chat_with_huggingface(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 8. chat_with_deepseek(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
#
#
####################
#
# Import necessary libraries
import json
import os
import time
from typing import List, Any, Optional, Tuple, Dict, Union
#
# Import 3rd-Party Libraries
import requests
from loguru import logger
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from tldw_Server_API.app.core.Chat.Chat_Deps import ChatAPIError
#
# Import Local libraries
from tldw_Server_API.app.core.config import load_and_log_configs
from tldw_Server_API.app.core.Utils.Utils import logging
from tldw_Server_API.app.core.Chat.Chat_Functions import ChatAuthenticationError, ChatRateLimitError, \
    ChatBadRequestError, ChatProviderError, ChatConfigurationError
#
#######################################################################################################################
# Function Definitions
#

# FIXME: Update to include full arguments

# --- Helper function for safe type conversion ---
def _safe_cast(value: Any, cast_to: type, default: Any = None) -> Any:
    """Safely casts value to specified type, returning default on failure."""
    if value is None:
        return default
    try:
        return cast_to(value)
    except (ValueError, TypeError):
        logging.warning(f"Could not cast '{value}' to {cast_to}. Using default: {default}")
        return default

def extract_text_from_segments(segments):
    logging.debug(f"Segments received: {segments}")
    logging.debug(f"Type of segments: {type(segments)}")

    text = ""

    if isinstance(segments, list):
        for segment in segments:
            logging.debug(f"Current segment: {segment}")
            logging.debug(f"Type of segment: {type(segment)}")
            if 'Text' in segment:
                text += segment['Text'] + " "
            else:
                logging.warning(f"Skipping segment due to missing 'Text' key: {segment}")
    else:
        logging.warning(f"Unexpected type of 'segments': {type(segments)}")

    return text.strip()


def _parse_data_url_for_multimodal(data_url: str) -> Optional[Tuple[str, str]]:
    """Parses a data URL (e.g., data:image/png;base64,xxxx) into (mime_type, base64_data)."""
    if data_url.startswith("data:") and ";base64," in data_url:
        try:
            header, b64_data = data_url.split(";base64,", 1)
            mime_type = header.split("data:", 1)[1]
            return mime_type, b64_data
        except Exception as e:
            logging.warning(f"Could not parse data URL: {data_url[:60]}... Error: {e}")
            return None
    logging.debug(f"Data URL did not match expected format: {data_url[:60]}...")
    return None


def get_openai_embeddings(input_data: str, model: str) -> List[float]:
    """
    Get embeddings for the input text from OpenAI API.
    Args:
        input_data (str): The input text to get embeddings for.
        model (str): The model to use for generating embeddings.
    Returns:
        List[float]: The embeddings generated by the API.
    """
    loaded_config_data = load_and_log_configs()
    api_key = loaded_config_data['openai_api']['api_key']

    if not api_key:
        logging.error("OpenAI Embeddings: API key not found or is empty")
        raise ValueError("OpenAI Embeddings: API Key Not Provided/Found in Config file or is empty")

    logging.debug(f"OpenAI Embeddings: Using API Key: {api_key[:5]}...{api_key[-5:]}")
    logging.debug(f"OpenAI Embeddings: Raw input data (first 500 chars): {str(input_data)[:500]}...")
    logging.debug(f"OpenAI Embeddings: Using model: {model}")

    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    request_data = {
        "input": input_data,
        "model": model,
    }
    try:
        logging.debug("OpenAI Embeddings: Posting request to embeddings API")
        response = requests.post('https://api.openai.com/v1/embeddings', headers=headers, json=request_data)
        logging.debug(f"Full API response data: {response}")
        if response.status_code == 200:
            response_data = response.json()
            if 'data' in response_data and len(response_data['data']) > 0:
                embedding = response_data['data'][0]['embedding']
                logging.debug("OpenAI Embeddings: Embeddings retrieved successfully")
                return embedding
            else:
                logging.warning("OpenAI Embeddings: Embedding data not found in the response")
                raise ValueError("OpenAI Embeddings: Embedding data not available in the response")
        else:
            logging.error(f"OpenAI Embeddings: request failed with status code {response.status_code}")
            logging.error(f"OpenAI Embeddings: Error response: {response.text}")
            # Propagate HTTPError to be caught by chat_api_call's handler (if this were called from there)
            # Or raise specific error if called directly
            response.raise_for_status() # This will raise HTTPError
            # Fallback if raise_for_status doesn't cover it (it should)
            raise ValueError(f"OpenAI Embeddings: Failed to retrieve. Status code: {response.status_code}")
    except requests.RequestException as e:
        logging.error(f"OpenAI Embeddings: Error making API request: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI Embeddings: Error making API request: {str(e)}")
    except Exception as e:
        logging.error(f"OpenAI Embeddings: Unexpected error: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI Embeddings: Unexpected error occurred: {str(e)}")


def chat_with_openai(
        input_data: List[Dict[str, Any]],  # Mapped from 'messages_payload'
        model: Optional[str] = None,  # Mapped from 'model'
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        system_message: Optional[str] = None,  # Mapped from 'system_message'
        temp: Optional[float] = None,  # Mapped from 'temp' (temperature)
        maxp: Optional[float] = None,  # Mapped from 'maxp' (top_p)
        streaming: Optional[bool] = False,  # Mapped from 'streaming'
        # New OpenAI specific parameters (and some from original ChatCompletionRequest schema)
        frequency_penalty: Optional[float] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        logprobs: Optional[bool] = None,  # True/False
        top_logprobs: Optional[int] = None,
        max_tokens: Optional[int] = None,  # This was already implicitly handled by config, now explicit
        n: Optional[int] = None,  # Number of completions
        presence_penalty: Optional[float] = None,
        response_format: Optional[Dict[str, str]] = None,  # e.g., {"type": "json_object"}
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        user: Optional[str] = None, # This is the 'user_identifier' mapped
        custom_prompt_arg: Optional[str] = None # Legacy
):
    """
    Sends a chat completion request to the OpenAI API.

    Args:
        input_data: List of message objects (OpenAI format).
        model: ID of the model to use.
        api_key: OpenAI API key.
        system_message: Optional system message to prepend.
        temp: Sampling temperature.
        maxp: Top-p (nucleus) sampling parameter.
        streaming: Whether to stream the response.
        frequency_penalty: Penalizes new tokens based on their existing frequency.
        logit_bias: Modifies the likelihood of specified tokens appearing.
        logprobs: Whether to return log probabilities of output tokens.
        top_logprobs: An integer between 0 and 5 specifying the number of most likely tokens to return at each token position.
        max_tokens: Maximum number of tokens to generate.
        n: How many chat completion choices to generate for each input message.
        presence_penalty: Penalizes new tokens based on whether they appear in the text so far.
        response_format: An object specifying the format that the model must output. e.g. {"type": "json_object"}.
        seed: This feature is in Beta. If specified, the system will make a best effort to sample deterministically.
        stop: Up to 4 sequences where the API will stop generating further tokens.
        tools: A list of tools the model may call.
        tool_choice: Controls which (if any) function is called by the model.
        user: A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
        custom_prompt_arg: Legacy, largely ignored.
        **kwargs: Catches any unexpected keyword arguments.
    """
    loaded_config_data = load_and_log_configs()
    openai_config = loaded_config_data.get('openai_api', {})

    final_api_key = api_key or openai_config.get('api_key')
    if not final_api_key:
        logging.error("OpenAI: API key is missing.")
        raise ChatConfigurationError(provider="openai", message="OpenAI API Key is required but not found.")

    log_key_display = f"{final_api_key[:5]}...{final_api_key[-5:]}" if final_api_key and len(final_api_key) > 9 else "Key Provided"
    logging.debug(f"OpenAI: Using API Key: {log_key_display}")

    # Resolve parameters: User-provided > Function arg default > Config default > Hardcoded default
    final_model = model if model is not None else openai_config.get('model', 'gpt-4o-mini')
    final_temp = temp if temp is not None else float(openai_config.get('temperature', 0.7))
    final_top_p = maxp if maxp is not None else float(
        openai_config.get('top_p', 0.95))  # 'maxp' from chat_api_call maps to 'top_p'

    final_streaming_cfg = openai_config.get('streaming', False)
    final_streaming = streaming if streaming is not None else \
        (str(final_streaming_cfg).lower() == 'true' if isinstance(final_streaming_cfg, str) else bool(final_streaming_cfg))

    final_max_tokens = max_tokens if max_tokens is not None else _safe_cast(openai_config.get('max_tokens'), int)

    if custom_prompt_arg:
        logging.warning(
            "OpenAI: 'custom_prompt_arg' was provided but is generally ignored if 'input_data' and 'system_message' are used correctly.")

    # Construct messages for OpenAI API
    api_messages = []
    has_system_message_in_input = any(msg.get("role") == "system" for msg in input_data)
    if system_message and not has_system_message_in_input:
        api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data)

    payload = {
        "model": final_model,
        "messages": api_messages,
        "stream": final_streaming,
    }
    # Add optional parameters if they have a value
    if final_temp is not None: payload["temperature"] = final_temp
    if final_top_p is not None: payload["top_p"] = final_top_p # OpenAI uses top_p
    if final_max_tokens is not None: payload["max_tokens"] = final_max_tokens
    if frequency_penalty is not None: payload["frequency_penalty"] = frequency_penalty
    if logit_bias is not None: payload["logit_bias"] = logit_bias
    if logprobs is not None: payload["logprobs"] = logprobs
    if top_logprobs is not None and payload.get("logprobs") is True:
        payload["top_logprobs"] = top_logprobs
    elif top_logprobs is not None:
         logging.warning("OpenAI: 'top_logprobs' provided but 'logprobs' is not true. 'top_logprobs' will be ignored.")
    if n is not None: payload["n"] = n
    if presence_penalty is not None: payload["presence_penalty"] = presence_penalty
    if response_format is not None: payload["response_format"] = response_format
    if seed is not None: payload["seed"] = seed
    if stop is not None: payload["stop"] = stop
    if tools is not None: payload["tools"] = tools
    if tools is not None: payload["tools"] = tools

    # Then conditionally add tool_choice:
    if payload.get("tools") and tool_choice is not None:
        payload["tool_choice"] = tool_choice
    elif tool_choice == "none":  # Allow "none" even if no tools are present
        payload["tool_choice"] = "none"
    if user is not None: payload["user"] = user # 'user' is OpenAI's user identifier field

    headers = {
        'Authorization': f'Bearer {final_api_key}',
        'Content-Type': 'application/json'
    }
    logging.debug(f"OpenAI Request Payload (excluding messages): {{k: v for k, v in payload.items() if k != 'messages'}}")

    api_url = openai_config.get('api_base_url', 'https://api.openai.com/v1').rstrip('/') + '/chat/completions'
    try:
        if final_streaming:
            logging.debug("OpenAI: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=payload, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"  # Adhere to SSE format for client
                        # OpenAI stream itself does not send a final [DONE] in this way,
                        # but our endpoint wrapper expects it.
                        # The actual DONE event should be data: [DONE]\n\n
                    except requests.exceptions.ChunkedEncodingError as e_chunk:
                        logging.error(f"OpenAI: ChunkedEncodingError during stream: {e_chunk}", exc_info=True)
                        error_content = json.dumps({"error": {"message": f"Stream connection error: {str(e_chunk)}",
                                                              "type": "openai_stream_error"}})
                        yield f"data: {error_content}\n\n"
                    except Exception as e_stream:
                        logging.error(f"OpenAI: Error during stream iteration: {e_stream}", exc_info=True)
                        error_content = json.dumps({"error": {"message": f"Stream iteration error: {str(e_stream)}",
                                                              "type": "openai_stream_error"}})
                        yield f"data: {error_content}\n\n"
                    finally:
                        # Ensure DONE is sent for the endpoint wrapper's logic
                        yield "data: [DONE]\n\n"
                        if response:
                            response.close()

                return stream_generator()

        else:  # Non-streaming
            logging.debug("OpenAI: Posting request (non-streaming)")
            retry_count = int(openai_config.get('api_retries', 3))
            retry_delay = float(openai_config.get('api_retry_delay', 1.0))  # Ensure float

            retry_strategy = Retry(
                total=retry_count,
                backoff_factor=retry_delay,
                status_forcelist=[429, 500, 502, 503, 504],
                allowed_methods=["POST"]  # Changed from method_whitelist
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                session.mount("http://", adapter)  # Though OpenAI is https
                response = session.post(api_url, headers=headers, json=payload,
                                        timeout=float(openai_config.get('api_timeout', 90.0)))

            logging.debug(f"OpenAI: Full API response status: {response.status_code}")
            response.raise_for_status()  # Raise HTTPError for 4xx/5xx AFTER retries
            response_data = response.json()
            logging.debug("OpenAI: Non-streaming request successful.")
            return response_data

    except requests.exceptions.HTTPError as e:
        error_content_text = "No response text"
        error_content_json = None
        if e.response is not None:
            logging.error(f"OpenAI Full Error Response (status {e.response.status_code}): {e.response.text}")
        else:
            logging.error(f"OpenAI HTTPError with no response object: {e}")
        raise
        # if e.response is not None:
        #     error_content_text = e.response.text
        #     try:
        #         error_content_json = e.response.json()
        #     except json.JSONDecodeError:
        #         pass
        # logging.error(
        #     f"OpenAI HTTPError {e.response.status_code if e.response is not None else 'Unknown'}. Text: {error_content_text}. JSON: {error_content_json}",
        #     exc_info=True)
        # raise
    except requests.exceptions.RequestException as e:
        logging.error(f"OpenAI RequestException: {e}", exc_info=True)
        raise
    except Exception as e: # Catch any other unexpected error
        logging.error(f"OpenAI: Unexpected error in chat_with_openai: {e}", exc_info=True)
        raise ChatProviderError(provider="openai", message=f"Unexpected error: {e}")


def chat_with_anthropic(
        input_data: List[Dict[str, Any]], # Mapped from 'messages_payload'
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_prompt: Optional[str] = None, # Mapped from 'system_message'
        temp: Optional[float] = None,
        topp: Optional[float] = None,       # Mapped from 'topp' (becomes top_p)
        topk: Optional[int] = None,
        streaming: Optional[bool] = False,
        max_tokens: Optional[int] = None,   # New: Anthropic uses 'max_tokens'
        stop_sequences: Optional[List[str]] = None, # New: Mapped from 'stop'
        tools: Optional[List[Dict[str, Any]]] = None, # New: Anthropic tool format
        # Anthropic doesn't typically use seed, response_format (for JSON object mode directly), n, user identifier, logit_bias,
        # presence_penalty, frequency_penalty, logprobs, top_logprobs in the same way as OpenAI.
        # tool_choice is usually implicit with tools or controlled differently.
        custom_prompt_arg: Optional[str] = None # Legacy
):
    # Assuming load_and_log_configs is defined elsewhere
    loaded_config_data = load_and_log_configs()
    anthropic_config = loaded_config_data.get('anthropic_api', {})
    final_api_key = api_key or anthropic_config.get('api_key')
    if not final_api_key:
        raise ChatConfigurationError(provider="anthropic", message="Anthropic API Key is required.")

    log_key = f"{final_api_key[:5]}...{final_api_key[-5:]}" if final_api_key and len(final_api_key) > 9 else "Key Provided"
    logging.debug(f"Anthropic: Using API Key: {log_key}")

    current_model = model or anthropic_config.get('model', 'claude-3-haiku-20240307')
    current_temp = temp if temp is not None else float(anthropic_config.get('temperature', 0.7))
    current_top_p = topp
    current_top_k = topk
    current_streaming_cfg = anthropic_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(current_streaming_cfg))

    # Use the passed max_tokens if available, else config, else a default
    default_max_tokens = int(anthropic_config.get('max_tokens_to_sample', anthropic_config.get('max_tokens', 4096)))
    current_max_tokens = max_tokens if max_tokens is not None else default_max_tokens


    anthropic_messages = []
    for msg in input_data:
        role = msg.get("role")
        content = msg.get("content")
        if role not in ["user", "assistant"]:
            logging.warning(f"Anthropic: Skipping message with unsupported role: {role}")
            continue
        # ... (multimodal content processing for Anthropic from your existing function) ...
        anthropic_content_parts = []
        if isinstance(content, str):
            anthropic_content_parts.append({"type": "text", "text": content})
        elif isinstance(content, list): # OpenAI content part list
            for part in content:
                part_type = part.get("type")
                if part_type == "text":
                    anthropic_content_parts.append({"type": "text", "text": part.get("text", "")})
                elif part_type == "image_url":
                    image_url_obj = part.get("image_url", {})
                    url_str = image_url_obj.get("url", "")
                    parsed_image = _parse_data_url_for_multimodal(url_str)
                    if parsed_image:
                        mime_type, b64_data = parsed_image
                        anthropic_content_parts.append({
                            "type": "image",
                            "source": {"type": "base64", "media_type": mime_type, "data": b64_data}
                        })
        if anthropic_content_parts:
             anthropic_messages.append({"role": role, "content": anthropic_content_parts})


    if not any(m['role'] == 'user' for m in anthropic_messages):
        raise ChatBadRequestError(provider="anthropic", message="No valid user messages found for Anthropic.")

    headers = {
        'x-api-key': final_api_key,
        'anthropic-version': anthropic_config.get('api_version', '2023-06-01'),
        'Content-Type': 'application/json'
    }
    data = {
        "model": current_model,
        "max_tokens": current_max_tokens, # Changed from max_tokens_to_sample to the parameter
        "messages": anthropic_messages,
        "stream": current_streaming,
    }
    if system_prompt is not None: data["system"] = system_prompt # Anthropic uses 'system' at the top level
    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_top_k is not None: data["top_k"] = current_top_k
    if stop_sequences is not None: data["stop_sequences"] = stop_sequences
    if tools is not None: data["tools"] = tools # Assuming 'tools' is already in Anthropic's required format

    api_url = anthropic_config.get('api_base_url', 'https://api.anthropic.com/v1').rstrip('/') + '/messages'
    logging.debug(f"Anthropic Request Payload (excluding messages): {{k: v for k, v in data.items() if k != 'messages'}}")

    try:
        retry_count = int(anthropic_config.get('api_retries', 3))
        retry_delay = float(anthropic_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay, status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        with requests.Session() as session:
            session.mount("https://", adapter)
            response = session.post(api_url, headers=headers, json=data, stream=current_streaming, timeout=180)
        response.raise_for_status()

        if current_streaming:
            logging.debug("Anthropic: Streaming response received. Normalizing to OpenAI SSE.")
            def stream_generator():
                completion_id = f"chatcmpl-anthropic-{time.time_ns()}"
                created_time = int(time.time())

                event_name = None
                data_buffer = []

                try:
                    for line_bytes in response.iter_lines():  # iter_lines gives bytes
                        line = line_bytes.decode('utf-8').strip()
                        if not line: continue # Skip keep-alive newlines
                        # Anthropic SSE has "event:" and "data:" lines
                        # Parse them and reformat
                        # Example (simplified, actual Anthropic events are more complex):
                        if line.startswith("data:"):
                            event_data_str = line[len("data:"):].strip()
                            try:
                                anthropic_event = json.loads(event_data_str)
                                # Extract delta and finish_reason based on anthropic_event['type']
                                # (e.g., 'content_block_delta', 'message_delta')
                                delta_content = None
                                finish_reason = None
                                if anthropic_event.get("type") == "content_block_delta":
                                    delta = anthropic_event.get("delta", {})
                                    if delta.get("type") == "text_delta":
                                        delta_content = delta.get("text")
                                elif anthropic_event.get("type") == "message_delta":
                                    finish_reason_anth = anthropic_event.get("delta", {}).get("stop_reason")
                                    if finish_reason_anth:
                                        finish_reason_map = {"end_turn": "stop", "max_tokens": "length", "stop_sequence": "stop", "tool_use": "tool_calls"}
                                        finish_reason = finish_reason_map.get(finish_reason_anth, finish_reason_anth)

                                if delta_content:
                                    sse_chunk = {"id": completion_id, "object": "chat.completion.chunk", "created": created_time, "model": current_model, "choices": [{"index": 0, "delta": {"content": delta_content}, "finish_reason": None}]}
                                    yield f"data: {json.dumps(sse_chunk)}\n\n"
                                if finish_reason:
                                    sse_chunk = {"id": completion_id, "object": "chat.completion.chunk", "created": created_time, "model": current_model, "choices": [{"index": 0, "delta": {}, "finish_reason": finish_reason}]}
                                    yield f"data: {json.dumps(sse_chunk)}\n\n"
                            except json.JSONDecodeError:
                                logging.warning(f"Anthropic Stream: Could not decode JSON: {event_data_str}")
                    yield "data: [DONE]\n\n"
                except requests.exceptions.ChunkedEncodingError as e: # ... error handling ...
                    logging.error(f"Anthropic: ChunkedEncodingError during stream: {e}", exc_info=True)
                    yield f"data: {json.dumps({'error': {'message': f'Stream connection error: {str(e)}', 'type': 'anthropic_stream_error'}})}\n\n"
                except Exception as e: # ... error handling ...
                    logging.error(f"Anthropic: Error during stream iteration: {e}", exc_info=True)
                    yield f"data: {json.dumps({'error': {'message': f'Stream iteration error: {str(e)}', 'type': 'anthropic_stream_error'}})}\n\n"
                finally:
                    yield "data: [DONE]\n\n" # Crucial for client
                    if response: response.close()
            return stream_generator()
        else:
            # ... (non-streaming logic remains the same) ...
            logging.debug("Anthropic: Non-streaming request successful.")
            response_data = response.json()
            logging.debug("Anthropic: Non-streaming request successful. Normalizing response.")
            assistant_content_parts = []
            if response_data.get("content"):
                for part in response_data.get("content", []):
                    if part.get("type") == "text":
                        assistant_content_parts.append(part.get("text", ""))
            full_assistant_content = "\n".join(assistant_content_parts).strip()
            finish_reason_map = {"end_turn": "stop", "max_tokens": "length", "stop_sequence": "stop", "tool_use": "tool_calls"} # Added tool_use
            openai_finish_reason = finish_reason_map.get(response_data.get("stop_reason"), response_data.get("stop_reason"))
            normalized_response = {
                "id": response_data.get("id", f"anthropic-{time.time_ns()}"),
                "object": "chat.completion",
                "created": int(time.time()),
                "model": response_data.get("model", current_model),
                "choices": [{"index": 0, "message": {"role": "assistant", "content": full_assistant_content},
                             "finish_reason": openai_finish_reason}],
                "usage": response_data.get("usage")
            }
            return normalized_response

    except requests.exceptions.HTTPError as e:
        # ... (error handling from your file, ensure provider is "anthropic") ...
        status_code = e.response.status_code if e.response is not None else 500
        error_text = e.response.text if e.response is not None else "No response text"
        if status_code == 401: raise ChatAuthenticationError(provider="anthropic", message=f"Auth failed. Detail: {error_text[:200]}") from e
        elif status_code == 429: raise ChatRateLimitError(provider="anthropic", message=f"Rate limit. Detail: {error_text[:200]}") from e
        elif 400 <= status_code < 500: raise ChatBadRequestError(provider="anthropic", message=f"Bad request ({status_code}). Detail: {error_text[:200]}") from e
        else: raise ChatProviderError(provider="anthropic", message=f"API error ({status_code}). Detail: {error_text[:200]}", status_code=status_code) from e
    except requests.exceptions.RequestException as e:
        raise ChatProviderError(provider="anthropic", message=f"Network error: {str(e)}", status_code=504) from e
    except Exception as e:
        logging.error(f"Anthropic: Unexpected error: {e}", exc_info=True)
        raise ChatProviderError(provider="anthropic", message=f"Unexpected error: {e}")


def chat_with_cohere(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_prompt: Optional[str] = None,
        temp: Optional[float] = None,
        streaming: Optional[bool] = False,
        topp: Optional[float] = None,
        topk: Optional[int] = None,
        max_tokens: Optional[int] = None,
        stop_sequences: Optional[List[str]] = None,
        seed: Optional[int] = None,
        num_generations: Optional[int] = None, # Only for non-streaming
        frequency_penalty: Optional[float] = None,
        presence_penalty: Optional[float] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        custom_prompt_arg: Optional[str] = None # Kept for legacy, but focus on structured input
):
    logging.debug(f"Cohere Chat: Request process starting for model '{model}' (Streaming: {streaming})")
    loaded_config_data = load_and_log_configs()
    cohere_config = loaded_config_data.get('cohere_api', loaded_config_data.get('API', {}).get('cohere', {}))

    final_api_key = api_key or cohere_config.get('api_key')
    if not final_api_key:
        raise ChatAuthenticationError(provider="cohere", message="Cohere API key is missing.")
    logging.debug(f"Cohere: Using API Key: {final_api_key[:5]}...{final_api_key[-5:]}")

    final_model = model or cohere_config.get('model', 'command-r')
    api_base_url = cohere_config.get('api_base_url', 'https://api.cohere.com').rstrip('/')
    # Using /v1/chat is standard for Cohere's current Chat API
    COHERE_CHAT_URL = f"{api_base_url}/v1/chat"

    # Timeout for each attempt, retries will extend total possible time
    timeout_seconds = float(cohere_config.get('api_timeout', 180.0)) # Increased default
    # For streaming, timeout usually applies to establishing connection and time between chunks.
    # The session timeout below will handle per-try timeout.

    headers = {
        "Authorization": f"Bearer {final_api_key}",
        "Content-Type": "application/json",
        "Accept": "text/event-stream" if streaming else "application/json",
        # Consider using a more recent API version or removing if not strictly needed, to get Cohere's latest defaults
        "Cohere-Version": cohere_config.get('api_version_date', "2024-05-13")
    }

    chat_history_for_cohere = []
    current_user_message_str = ""
    preamble_str = system_prompt or "" # 'preamble' is Cohere's term for system prompt

    temp_messages = list(input_data) # Make a mutable copy

    if not preamble_str and temp_messages and temp_messages[0]['role'] == 'system':
        preamble_str = temp_messages.pop(0)['content']
        logging.debug(f"Cohere: Using system message from input_data as preamble: '{preamble_str[:100]}...'")

    if not temp_messages: # Ensure there are messages left after potential preamble extraction
        # If custom_prompt_arg is provided and meaningful as a user query, consider using it.
        # For now, raising an error if no user/assistant messages remain.
        if custom_prompt_arg:
            current_user_message_str = custom_prompt_arg
            logging.warning("Cohere: No user/assistant messages in input_data, using custom_prompt_arg as user message.")
        else:
            raise ChatBadRequestError(provider="cohere",
                                      message="No user/assistant messages found for Cohere chat after processing system message.")
    elif temp_messages[-1]['role'] == 'user':
        last_msg_content = temp_messages[-1]['content']
        # Handle cases where content might be a list (e.g. multimodal, though Cohere handles this differently)
        if isinstance(last_msg_content, list): # Assuming OpenAI structure with type:text
            current_user_message_str = next((part['text'] for part in last_msg_content if part.get('type') == 'text'), "")
        else:
            current_user_message_str = str(last_msg_content)
        chat_history_for_cohere = temp_messages[:-1] # All but the last user message
    else: # Last message is not 'user', problematic for Cohere's /chat
        current_user_message_str = custom_prompt_arg or "Please respond." # Fallback user message
        chat_history_for_cohere = temp_messages # Keep all as history, and append the placeholder user message
        logging.warning(
            f"Cohere: Last message in payload was not 'user'. Using fallback user message: '{current_user_message_str}'.")

    # Append custom_prompt_arg to the current user message if it exists
    if custom_prompt_arg and current_user_message_str != custom_prompt_arg: # Avoid duplication if already used as fallback
        current_user_message_str += f"\n{custom_prompt_arg}"
        logging.debug(f"Cohere: Appended custom_prompt_arg to current user message.")


    if not current_user_message_str.strip():
        raise ChatBadRequestError(provider="cohere", message="Current user message for Cohere is empty after processing.")

    transformed_history = []
    for msg in chat_history_for_cohere:
        role = msg.get('role', '').lower()
        content = msg.get('content', '')
        if isinstance(content, list): # Extract text if content is a list of parts
            content = next((part['text'] for part in content if part.get('type') == 'text'), "")

        if role == "user":
            transformed_history.append({"role": "USER", "message": str(content)}) # Cohere uses "USER"
        elif role == "assistant":
            transformed_history.append({"role": "CHATBOT", "message": str(content)}) # Cohere uses "CHATBOT"
        # System messages are handled by preamble

    payload: Dict[str, Any] = {
        "model": final_model,
        "message": current_user_message_str
    }
    # Add parameters to payload only if they are not None or have meaningful values
    if transformed_history: payload["chat_history"] = transformed_history
    if preamble_str: payload["preamble"] = preamble_str
    if temp is not None: payload["temperature"] = temp
    if topp is not None: payload["p"] = topp
    if topk is not None: payload["k"] = topk
    if max_tokens is not None: payload["max_tokens"] = max_tokens
    if stop_sequences: payload["stop_sequences"] = stop_sequences
    if seed is not None: payload["seed"] = seed
    if frequency_penalty is not None: payload["frequency_penalty"] = frequency_penalty
    if presence_penalty is not None: payload["presence_penalty"] = presence_penalty
    if tools: payload["tools"] = tools # Assuming 'tools' is already in Cohere's expected format

    if streaming:
        payload["stream"] = True
    else:
        # For non-streaming, 'stream: false' can be in payload or omitted.
        # Cohere's API defaults to non-streaming if 'stream' is not true.
        # To be explicit, we can add it.
        payload["stream"] = False
        if num_generations is not None and num_generations > 0 : # num_generations is for non-streaming
            payload["num_generations"] = num_generations
        elif num_generations is not None and num_generations <=0:
             logging.warning("Cohere: 'num_generations' must be > 0. Ignoring.")


    logging.debug(f"Cohere Request Payload: {json.dumps(payload, indent=2)}")
    logging.debug(f"Cohere Request URL: {COHERE_CHAT_URL}")

    # --- Retry Mechanism ---
    session = requests.Session()
    retry_count = int(cohere_config.get('api_retries', 3))
    retry_delay = float(cohere_config.get('api_retry_delay', 1.0)) # Ensure float for backoff_factor

    retry_strategy = Retry(
        total=retry_count,
        backoff_factor=retry_delay,
        status_forcelist=[429, 500, 502, 503, 504], # Standard retry statuses
        allowed_methods=["POST"] # Retry only for POST requests
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    # --- End Retry Mechanism ---

    try:
        if streaming:
            # For streaming, the session.post will use the retry for initial connection.
            # The timeout applies to each attempt for connection and then for pauses in stream.
            response = session.post(COHERE_CHAT_URL, headers=headers, json=payload, stream=True, timeout=timeout_seconds)
            response.raise_for_status() # Check for HTTP errors on initial connection
            logging.debug("Cohere: Streaming response connection established.")

            def stream_generator_cohere_text_chunks(response_iterator): # Removed unused model_name_for_event
                stream_properly_closed = False
                accumulated_text_for_log = []
                try:
                    for line_bytes in response_iterator:
                        if not line_bytes: continue
                        decoded_line = line_bytes.decode('utf-8').strip()
                        if not decoded_line: continue

                        # Cohere's /v1/chat SSE format:
                        # It often sends events like:
                        # event: text-generation\ndata: {"text": "...", ...}\n\n
                        # event: stream-end\ndata: {"finish_reason": "...", ...}\n\n
                        # Sometimes, there's no explicit 'event:' line, and the data line itself contains 'event_type'.
                        # We need to handle lines starting with 'data:' primarily.

                        if decoded_line.startswith("data:"):
                            json_data_str = decoded_line[len("data:"):].strip()
                            if not json_data_str: continue
                            try:
                                cohere_event = json.loads(json_data_str)
                                event_type = cohere_event.get("event_type")

                                if event_type == "text-generation":
                                    text_chunk = cohere_event.get("text")
                                    if text_chunk:
                                        accumulated_text_for_log.append(text_chunk)
                                        yield text_chunk
                                elif event_type == "stream-end":
                                    stream_properly_closed = True
                                    final_response_details = cohere_event.get("response", {})
                                    finish_reason = final_response_details.get("finish_reason") or cohere_event.get("finish_reason", "UNKNOWN")
                                    logging.info(f"Cohere stream: 'stream-end' event. Finish: {finish_reason}. Fragments: {len(accumulated_text_for_log)}")
                                    return
                                elif event_type == "stream-start": # Cohere sends this
                                    logging.debug(f"Cohere stream: 'stream-start' event. Gen ID: {cohere_event.get('generation_id')}")
                                elif event_type: # Log other known event types if curious
                                     logging.debug(f"Cohere stream event type: {event_type}, data: {cohere_event}")

                            except json.JSONDecodeError:
                                logging.warning(f"Cohere Stream: JSON decode error for data: '{json_data_str}' from line: '{decoded_line}'")
                        elif decoded_line.startswith("event:"):
                            # This line just declares the event type, data line follows.
                            # logging.trace(f"Cohere stream saw event line: {decoded_line}")
                            pass # Handled by the data line's event_type
                        else:
                            logging.warning(f"Cohere Stream: Unexpected line format: '{decoded_line}'")

                except requests.exceptions.ChunkedEncodingError as e:
                    logging.warning(f"Cohere stream: ChunkedEncodingError: {e}. Stream may have been interrupted.")
                except Exception as e_stream:
                    logging.error(f"Cohere stream: Error during streaming: {e_stream}", exc_info=True)
                finally:
                    if not stream_properly_closed:
                        logging.warning("Cohere stream generator loop finished without explicit 'stream-end'.")
                    logging.debug(f"Cohere stream_generator for {final_model} finished. Total text: {''.join(accumulated_text_for_log)[:100]}...")
                    if response: response.close()
            return stream_generator_cohere_text_chunks(response.iter_lines())
        else:  # Non-streaming
            # The session.post will use the retry strategy and timeout for each attempt.
            response = session.post(COHERE_CHAT_URL, headers=headers, json=payload, stream=False, timeout=timeout_seconds)
            # No params={"stream": "false"} needed; payload["stream"] = False handles it.
            response.raise_for_status() # Will raise HTTPError for bad responses (4xx or 5xx) after retries
            response_data = response.json()
            logging.debug(f"Cohere non-streaming response data: {json.dumps(response_data, indent=2)}")

            # ---- Standard OpenAI-like Response Mapping ----
            # Based on Cohere /v1/chat non-streaming response structure:
            # { "text": "...", "generation_id": "...", "citations": [...], "documents": [...],
            #   "is_search_required": bool, "search_queries": [...], "search_results": [...],
            #   "finish_reason": "...", "tool_calls": [...], "chat_history": [...], (returned chat history)
            #   "meta": { "api_version": {...}, "billed_units": {"input_tokens": X, "output_tokens": Y}}}

            chat_id = response_data.get("generation_id", f"chatcmpl-cohere-{time.time_ns()}")
            created_timestamp = int(time.time())
            choices_payload = []
            finish_reason = response_data.get("finish_reason", "stop") # Default, Cohere provides this

            if response_data.get("text"): # Standard text response
                choices_payload.append({
                    "message": {"role": "assistant", "content": response_data["text"]},
                    "finish_reason": finish_reason, "index": 0
                })
            elif response_data.get("tool_calls"): # Tool usage
                openai_like_tool_calls = []
                for tc in response_data.get("tool_calls", []):
                    openai_like_tool_calls.append({
                        "id": f"call_{tc.get('name', 'tool')}_{time.time_ns()}",
                        "type": "function", # Assuming Cohere tools map to functions
                        "function": {
                            "name": tc.get("name"),
                            "arguments": json.dumps(tc.get("parameters", {}))
                        }
                    })
                choices_payload.append({
                    "message": {"role": "assistant", "content": None, "tool_calls": openai_like_tool_calls},
                    "finish_reason": "tool_calls", "index": 0
                })
            else: # Fallback for unexpected empty response
                logging.warning(f"Cohere non-streaming response missing 'text' or 'tool_calls': {response_data}")
                choices_payload.append({
                    "message": {"role": "assistant", "content": ""},
                    "finish_reason": finish_reason, "index": 0
                })

            usage_data = None
            meta = response_data.get("meta")
            if meta and meta.get("billed_units"):
                billed_units = meta["billed_units"]
                prompt_tokens = billed_units.get("input_tokens")
                completion_tokens = billed_units.get("output_tokens")
                # search_units = billed_units.get("search_units") # if you track this
                if prompt_tokens is not None and completion_tokens is not None:
                    usage_data = {
                        "prompt_tokens": prompt_tokens,
                        "completion_tokens": completion_tokens,
                        "total_tokens": prompt_tokens + completion_tokens
                    }

            openai_compatible_response = {
                "id": chat_id, "object": "chat.completion", "created": created_timestamp,
                "model": final_model, "choices": choices_payload,
            }
            if usage_data: openai_compatible_response["usage"] = usage_data
            return openai_compatible_response

    except requests.exceptions.HTTPError as e:
        status_code = getattr(e.response, 'status_code', 500)
        error_text = getattr(e.response, 'text', str(e))
        logging.error(f"Cohere API call HTTPError to {COHERE_CHAT_URL} status {status_code}. Details: {error_text[:500]}", exc_info=False)
        if status_code == 401:
            raise ChatAuthenticationError(provider="cohere", message=f"Authentication failed. Detail: {error_text[:200]}")
        elif status_code == 429:
            raise ChatRateLimitError(provider="cohere", message=f"Rate limit exceeded. Detail: {error_text[:200]}")
        elif 400 <= status_code < 500:
            raise ChatBadRequestError(provider="cohere", message=f"Bad request (Status {status_code}). Detail: {error_text[:200]}")
        else: # 5xx
            raise ChatProviderError(provider="cohere", message=f"Server error (Status {status_code}). Detail: {error_text[:200]}", status_code=status_code)
    except requests.exceptions.RequestException as e: # Includes ReadTimeout, ConnectionError etc.
        logging.error(f"Cohere API request failed (network error) for {COHERE_CHAT_URL}: {e}", exc_info=True)
        # This will catch the ReadTimeout after retries are exhausted
        raise ChatProviderError(provider="cohere", message=f"Network error after retries: {e}", status_code=504) # 504 for gateway timeout like
    except Exception as e:
        logging.error(f"Cohere API call: Unexpected error: {e}", exc_info=True)
        if not isinstance(e, ChatAPIError):
            raise ChatAPIError(provider="cohere", message=f"Unexpected error in Cohere API call: {e}")
        else:
            raise
    finally:
        if session: # Ensure session is closed
            session.close()


def chat_with_deepseek(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_message: Optional[str] = None,
        temp: Optional[float] = None,
        streaming: Optional[bool] = False,
        topp: Optional[float] = None,  # top_p
        # New OpenAI-compatible params for DeepSeek
        max_tokens: Optional[int] = None,
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        response_format: Optional[Dict[str, str]] = None,  # If supported
        n: Optional[int] = None,  # If supported
        user: Optional[str] = None,  # If supported
        tools: Optional[List[Dict[str, Any]]] = None,  # If supported
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,  # If supported
        logit_bias: Optional[Dict[str, float]] = None,  # If supported
        custom_prompt_arg: Optional[str] = None  # Legacy
):
    loaded_config_data = load_and_log_configs()
    deepseek_config = loaded_config_data.get('deepseek_api', {})
    final_api_key = api_key or deepseek_config.get('api_key')
    if not final_api_key:
        raise ChatConfigurationError(provider="deepseek", message="DeepSeek API Key required.")

    # ... (logging key, model, temp, streaming, top_p setup) ...
    log_key = f"{final_api_key[:5]}...{final_api_key[-5:]}" if final_api_key and len(
        final_api_key) > 9 else "Key Provided"
    logging.debug(f"DeepSeek: Using API Key: {log_key}")
    current_model = model or deepseek_config.get('model', 'deepseek-chat')  # Or deepseek-coder
    current_temp = temp if temp is not None else float(deepseek_config.get('temperature', 0.1))
    current_top_p = topp  # Deepseek uses top_p
    current_streaming_cfg = deepseek_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(
            current_streaming_cfg))

    current_max_tokens = max_tokens if max_tokens is not None else _safe_cast(deepseek_config.get('max_tokens'), int)

    api_messages = []
    if system_message:
        api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data)

    headers = {'Authorization': f'Bearer {final_api_key}', 'Content-Type': 'application/json'}
    data = {
        "model": current_model, "messages": api_messages, "stream": current_streaming,
    }
    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_max_tokens is not None: data["max_tokens"] = current_max_tokens
    if seed is not None: data["seed"] = seed
    if stop is not None: data["stop"] = stop
    if logprobs is not None: data["logprobs"] = logprobs  # DeepSeek uses 'logprobs' (boolean)
    if top_logprobs is not None and data.get("logprobs"): data["top_logprobs"] = top_logprobs
    if presence_penalty is not None: data["presence_penalty"] = presence_penalty
    if frequency_penalty is not None: data["frequency_penalty"] = frequency_penalty
    if response_format is not None: data["response_format"] = response_format
    if n is not None: data["n"] = n
    if user is not None: data["user"] = user
    if tools is not None: data["tools"] = tools
    if tool_choice is not None: data["tool_choice"] = tool_choice
    if logit_bias is not None: data["logit_bias"] = logit_bias

    api_url = deepseek_config.get('api_base_url', 'https://api.deepseek.com').rstrip('/') + '/chat/completions'
    logging.debug(
        f"DeepSeek Request Payload (excluding messages): {{k: v for k, v in data.items() if k != 'messages'}}")

    try:
        if current_streaming:
            # ... (OpenAI-like streaming logic, use "DeepSeek" in logs) ...
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip(): yield line + "\n\n"
                    # ... (error handling for stream) ...
                    finally:
                        yield "data: [DONE]\n\n"
                        if response: response.close()

                return stream_generator()
        else:
            # ... (non-streaming, retry) ...
            adapter = HTTPAdapter(max_retries=Retry(total=int(deepseek_config.get('api_retries', 3)),
                                                    backoff_factor=float(deepseek_config.get('api_retry_delay', 1)),
                                                    status_forcelist=[429, 500, 502, 503, 504],
                                                    allowed_methods=["POST"]))
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            return response.json()
    except requests.exceptions.HTTPError as e:  # ... error handling ...
        raise
    except Exception as e:  # ... error handling ...
        raise ChatProviderError(provider="deepseek", message=f"Unexpected error: {e}")


def chat_with_google(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_message: Optional[str] = None,  # -> system_instruction
        temp: Optional[float] = None,  # -> temperature
        streaming: Optional[bool] = False,
        topp: Optional[float] = None,  # -> topP
        topk: Optional[int] = None,  # -> topK
        max_output_tokens: Optional[int] = None,  # from max_tokens
        stop_sequences: Optional[List[str]] = None,  # from stop
        candidate_count: Optional[int] = None,  # from n
        response_format: Optional[Dict[str, str]] = None,  # for response_mime_type
        # Gemini doesn't directly take seed, user_id, logit_bias, presence/freq_penalty, logprobs via REST in the same way.
        # Tools are handled via a 'tools' field in the payload, with a specific format.
        tools: Optional[List[Dict[str, Any]]] = None,  # Gemini 'tools' config
        custom_prompt_arg: Optional[str] = None
):
    loaded_config_data = load_and_log_configs()
    google_config = loaded_config_data.get('google_api', {})
    # ... (api key, model, temp, streaming, topP, topK setup) ...
    final_api_key = api_key or google_config.get('api_key')
    if not final_api_key: raise ChatConfigurationError(provider="google", message="Google API Key required.")
    current_model = model or google_config.get('model', 'gemini-1.5-flash-latest')
    # ... other param resolutions ...
    current_streaming_cfg = google_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(
            current_streaming_cfg))

    gemini_contents = []
    # ... (message transformation from input_data to gemini_contents) ...
    for msg in input_data:
        role = msg.get("role")
        content = msg.get("content")
        gemini_role = "user" if role == "user" else "model" if role == "assistant" else None
        if not gemini_role: continue
        gemini_parts = []
        if isinstance(content, str):
            gemini_parts.append({"text": content})
        elif isinstance(content, list):
            for part_obj in content:
                if part_obj.get("type") == "text":
                    gemini_parts.append({"text": part_obj.get("text", "")})
                elif part_obj.get("type") == "image_url":
                    parsed_image = _parse_data_url_for_multimodal(part_obj.get("image_url", {}).get("url", ""))
                    if parsed_image: gemini_parts.append(
                        {"inline_data": {"mime_type": parsed_image[0], "data": parsed_image[1]}})
        if gemini_parts: gemini_contents.append({"role": gemini_role, "parts": gemini_parts})

    generation_config = {}
    if temp is not None: generation_config["temperature"] = temp
    if topp is not None: generation_config["topP"] = topp
    if topk is not None: generation_config["topK"] = topk
    if max_output_tokens is not None: generation_config["maxOutputTokens"] = max_output_tokens
    if stop_sequences is not None: generation_config["stopSequences"] = stop_sequences
    if candidate_count is not None: generation_config["candidateCount"] = candidate_count
    if response_format and response_format.get("type") == "json_object":
        generation_config["responseMimeType"] = "application/json"

    payload = {"contents": gemini_contents}
    if generation_config: payload["generationConfig"] = generation_config
    if system_message: payload["system_instruction"] = {"parts": [{"text": system_message}]}
    if tools: payload["tools"] = tools  # Assuming 'tools' is in Gemini's specific format

    stream_suffix = ":streamGenerateContent?alt=sse" if current_streaming else ":generateContent"
    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{current_model}{stream_suffix}"
    headers = {'x-goog-api-key': final_api_key, 'Content-Type': 'application/json'}
    logging.debug(f"Google Gemini Request Payload: {payload}")

    try:
        # ... (retry logic) ...
        adapter = HTTPAdapter(max_retries=Retry(total=int(google_config.get('api_retries', 3)),
                                                backoff_factor=float(google_config.get('api_retry_delay', 1)),
                                                status_forcelist=[429, 500, 503], allowed_methods=["POST"]))
        with requests.Session() as session:
            session.mount("https://", adapter)
            response = session.post(api_url, headers=headers, json=payload, stream=current_streaming, timeout=180)
        response.raise_for_status()

        if current_streaming:
            logging.debug("Google Gemini: Streaming response received.")

            def stream_generator():
                try:
                    for line in response.iter_lines(decode_unicode=True):
                        if line and line.strip().startswith('data:'):
                            json_str = line.strip()[len('data:'):]
                            try:
                                data_chunk_outer = json.loads(json_str)
                                # Gemini stream chunks are sometimes lists, sometimes single objects
                                data_chunks_to_process = data_chunk_outer if isinstance(data_chunk_outer, list) else [
                                    data_chunk_outer]

                                for data_chunk in data_chunks_to_process:
                                    chunk_text = ""
                                    finish_reason = None  # Check for finish reason in Gemini stream
                                    tool_calls_delta = None

                                    candidates = data_chunk.get('candidates', [])
                                    if candidates:
                                        candidate = candidates[0]
                                        # Text content
                                        if candidate.get('content', {}).get('parts', []):
                                            for part in candidate['content']['parts']:
                                                if 'text' in part:
                                                    chunk_text += part.get('text', '')
                                                # Check for functionCall delta if Gemini supports streaming them this way
                                                if 'functionCall' in part:
                                                    # This needs careful mapping to OpenAI's tool_calls delta
                                                    # For now, just logging it if complex
                                                    logging.debug(
                                                        f"Gemini Stream: Received functionCall part: {part['functionCall']}")
                                                    # Example: if part['functionCall'] has 'name' and 'args'
                                                    # tool_calls_delta = [{"index": 0, "id": "call_SOMEID", "type": "function", "function": {"name": part['functionCall']['name'], "arguments": part['functionCall']['args']}}]

                                        # Finish reason
                                        raw_finish_reason = candidate.get("finishReason")
                                        if raw_finish_reason:
                                            finish_reason_map = {"MAX_TOKENS": "length", "STOP": "stop",
                                                                 "SAFETY": "content_filter",
                                                                 "RECITATION": "content_filter", "OTHER": "error",
                                                                 "TOOL_CODE_NOT_FOUND": "error"}  # Simplified
                                            finish_reason = finish_reason_map.get(raw_finish_reason,
                                                                                  raw_finish_reason.lower())

                                    if chunk_text or tool_calls_delta:  # If there's text or tool call delta
                                        delta_payload = {}
                                        if chunk_text: delta_payload["content"] = chunk_text
                                        if tool_calls_delta: delta_payload[
                                            "tool_calls"] = tool_calls_delta  # This needs to be OpenAI's delta format for tool_calls

                                        sse_chunk = {'choices': [{'delta': delta_payload,
                                                                  "finish_reason": finish_reason if finish_reason else None,
                                                                  "index": 0}]}
                                        # Add common SSE fields if needed by client: id, object, created, model
                                        yield f"data: {json.dumps(sse_chunk)}\n\n"
                                    elif finish_reason:  # Only finish reason
                                        sse_chunk = {
                                            'choices': [{'delta': {}, "finish_reason": finish_reason, "index": 0}]}
                                        yield f"data: {json.dumps(sse_chunk)}\n\n"

                            except json.JSONDecodeError:
                                logging.warning(f"Google Gemini: Could not decode JSON line: {json_str}")
                    yield "data: [DONE]\n\n"
                # ... (error handling for stream) ...
                finally:
                    if response: response.close()

            return stream_generator()
        else:
            response_data = response.json()
            logging.debug("Google Gemini: Non-streaming request successful.")
            assistant_content = ""
            finish_reason = "unknown"
            tool_calls = None  # For non-streaming

            if response_data.get("candidates"):
                candidate = response_data["candidates"][0]
                if candidate.get("content", {}).get("parts"):
                    parts = candidate["content"]["parts"]
                    for part in parts:
                        if "text" in part:
                            assistant_content += part.get("text", "")
                        if "functionCall" in part:  # Handle non-streaming tool calls
                            if tool_calls is None: tool_calls = []
                            # Map Gemini functionCall to OpenAI tool_calls format
                            # This needs a unique ID for each call.
                            tool_calls.append({
                                "id": f"call_gemini_{time.time_ns()}_{len(tool_calls)}",  # Placeholder ID
                                "type": "function",
                                "function": {
                                    "name": part["functionCall"].get("name"),
                                    "arguments": json.dumps(part["functionCall"].get("args", {}))
                                    # Ensure args is stringified JSON
                                }
                            })

                raw_finish_reason = candidate.get("finishReason")
                if raw_finish_reason:
                    finish_reason_map = {"MAX_TOKENS": "length", "STOP": "stop", "SAFETY": "content_filter",
                                         "RECITATION": "content_filter", "OTHER": "error",
                                         "TOOL_CODE_NOT_FOUND": "error", "FUNCTION_CALL": "tool_calls"}
                    finish_reason = finish_reason_map.get(raw_finish_reason, raw_finish_reason.lower())

            message_content = {"role": "assistant", "content": assistant_content.strip()}
            if tool_calls:
                message_content["tool_calls"] = tool_calls
                if not assistant_content.strip():  # If only tool_calls, content might be null for OpenAI spec
                    message_content["content"] = None

            normalized_response = {
                "id": f"gemini-{time.time_ns()}", "object": "chat.completion", "created": int(time.time()),
                "model": current_model,
                "choices": [{"index": 0, "message": message_content, "finish_reason": finish_reason}],
                "usage": {
                    "prompt_tokens": response_data.get("usageMetadata", {}).get("promptTokenCount"),
                    "completion_tokens": response_data.get("usageMetadata", {}).get("candidatesTokenCount"),
                    "total_tokens": response_data.get("usageMetadata", {}).get("totalTokenCount")}
            }
            return normalized_response
    except requests.exceptions.HTTPError as e:  # ... error handling ...
        raise
    except Exception as e:  # ... error handling ...
        raise ChatProviderError(provider="google", message=f"Unexpected error: {e}")



# https://console.groq.com/docs/quickstart
def chat_with_groq(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_message: Optional[str] = None,
        temp: Optional[float] = None,
        maxp: Optional[float] = None,  # top_p
        streaming: Optional[bool] = False,
        max_tokens: Optional[int] = None,
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        response_format: Optional[Dict[str, str]] = None,
        n: Optional[int] = None,
        user: Optional[str] = None,  # user_identifier
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        custom_prompt_arg: Optional[str] = None  # Legacy
):
    loaded_config_data = load_and_log_configs()
    groq_config = loaded_config_data.get('groq_api', {})
    final_api_key = api_key or groq_config.get('api_key')
    if not final_api_key:
        raise ChatConfigurationError(provider="groq", message="Groq API Key required.")

    # ... (logging key, model, temp, streaming setup as before) ...
    log_key = f"{final_api_key[:5]}...{final_api_key[-5:]}" if final_api_key and len(
        final_api_key) > 9 else "Key Provided"
    logging.debug(f"Groq: Using API Key: {log_key}")

    current_model = model or groq_config.get('model', 'llama3-8b-8192')
    current_temp = temp if temp is not None else float(groq_config.get('temperature', 0.2))
    current_top_p = maxp  # Groq uses top_p
    current_streaming_cfg = groq_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(
            current_streaming_cfg))

    current_max_tokens = max_tokens if max_tokens is not None else _safe_cast(groq_config.get('max_tokens'), int)

    api_messages = []
    if system_message:
        api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data)

    headers = {'Authorization': f'Bearer {final_api_key}', 'Content-Type': 'application/json'}
    data = {
        "model": current_model, "messages": api_messages, "stream": current_streaming,
    }
    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_max_tokens is not None: data["max_tokens"] = current_max_tokens
    if seed is not None: data["seed"] = seed
    if stop is not None: data["stop"] = stop
    if response_format is not None: data["response_format"] = response_format
    if n is not None: data["n"] = n
    if user is not None: data["user"] = user
    if tools is not None: data["tools"] = tools
    if tool_choice is not None: data["tool_choice"] = tool_choice
    if logit_bias is not None: data["logit_bias"] = logit_bias
    if presence_penalty is not None: data["presence_penalty"] = presence_penalty
    if frequency_penalty is not None: data["frequency_penalty"] = frequency_penalty
    if logprobs is not None: data["logprobs"] = logprobs
    if top_logprobs is not None and data.get("logprobs") is True: data["top_logprobs"] = top_logprobs

    api_url = groq_config.get('api_base_url', 'https://api.groq.com/openai/v1').rstrip('/') + '/chat/completions'
    logging.debug(f"Groq Request Payload (excluding messages): {{k: v for k, v in data.items() if k != 'messages'}}")
    try:
        if current_streaming:
            # ... (OpenAI-like streaming logic, ensure "Groq" in logs) ...
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip(): yield line + "\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:  # ... error handling ...
                        logging.error(f"Groq: ChunkedEncodingError: {e}", exc_info=True)
                        yield f"data: {json.dumps({'error': {'message': f'Stream error: {str(e)}', 'type': 'groq_stream_error'}})}\n\n"
                    except Exception as e:  # ... error handling ...
                        logging.error(f"Groq: Stream iteration error: {e}", exc_info=True)
                        yield f"data: {json.dumps({'error': {'message': f'Stream iteration error: {str(e)}', 'type': 'groq_stream_error'}})}\n\n"
                    finally:
                        yield "data: [DONE]\n\n"
                        if response: response.close()

                return stream_generator()
        else:
            # ... (non-streaming logic, retry) ...
            retry_count = int(groq_config.get('api_retries', 3))  # ... retry setup ...
            adapter = HTTPAdapter(
                max_retries=Retry(total=retry_count, backoff_factor=float(groq_config.get('api_retry_delay', 1)),
                                  status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"]))
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            return response.json()
    except requests.exceptions.HTTPError as e:  # ... error handling ...
        raise
    except Exception as e:  # ... error handling ...
        raise ChatProviderError(provider="groq", message=f"Unexpected error: {e}")


def chat_with_huggingface(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,  # This is the model_id like "Org/ModelName"
        api_key: Optional[str] = None,
        system_message: Optional[str] = None, # Renamed from system_prompt for clarity if it maps to HF system
        temp: Optional[float] = None,
        streaming: Optional[bool] = False,
        top_p: Optional[float] = None,
        top_k: Optional[int] = None,
        max_tokens: Optional[int] = None, # Maps to max_new_tokens for some TGI, or max_tokens for OpenAI compatible
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        response_format: Optional[Dict[str, str]] = None,
        num_return_sequences: Optional[int] = None,  # Mapped from 'n'
        user: Optional[str] = None, # OpenAI compatible user field
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        logit_bias: Optional[Dict[str, float]] = None, # OpenAI compatible
        presence_penalty: Optional[float] = None, # OpenAI compatible name
        frequency_penalty: Optional[float] = None, # OpenAI compatible name
        logprobs: Optional[bool] = None, # OpenAI compatible name
        top_logprobs: Optional[int] = None, # OpenAI compatible name
        custom_prompt_arg: Optional[str] = None  # Legacy
):
    logging.debug(f"HuggingFace Chat: Request process starting for model '{model}' (Streaming: {streaming})")
    loaded_config_data = load_and_log_configs()
    hf_config = loaded_config_data.get('huggingface_api', loaded_config_data.get('API', {}).get('huggingface', {}))

    final_api_key = api_key or hf_config.get('api_key')
    if final_api_key:
        log_key_display = f"{final_api_key[:5]}...{final_api_key[-5:]}" if len(final_api_key) > 9 else "Key Provided"
        logging.debug(f"HuggingFace: Using API Key: {log_key_display}")
    else:
        logging.warning("HuggingFace: API key is missing. Public Inference API or unsecured TGI assumed.")

    headers = {"Content-Type": "application/json"}
    if final_api_key:
        headers["Authorization"] = f"Bearer {final_api_key}"

    final_model_for_payload = model or hf_config.get('model_id') or hf_config.get('model')
    if not final_model_for_payload:
        raise ChatConfigurationError(provider="huggingface",
                                     message="HuggingFace model ID is required (must be passed as 'model' or configured).")
    logging.info(f"HuggingFace: Using model_id for payload: {final_model_for_payload}")

    # --- URL Construction ---
    api_url: str
    use_router_url_format_str = str(hf_config.get('use_router_url_format', "False")).lower()

    if use_router_url_format_str == "true":
        # This format explicitly puts the model in the URL path.
        # User must ensure router_base_url and model_id result in a valid endpoint.
        router_base = hf_config.get('router_base_url', 'https://router.huggingface.co/hf-inference').rstrip('/')
        model_path_part = final_model_for_payload.strip('/')
        chat_path = hf_config.get('api_chat_path', 'v1/chat/completions').lstrip('/')
        # Constructs URL like: {router_base}/models/{model_path_part}/{chat_path}
        api_url = f"{router_base}/models/{model_path_part}/{chat_path}"
        logging.info(f"HuggingFace: Using explicit 'use_router_url_format=true'. Target URL: {api_url}")
    else: # use_router_url_format is false, standard URL construction
        configured_api_base_url = hf_config.get('api_base_url')
        # Default chat path can be just "chat/completions" if base_url includes /v1, or "v1/chat/completions" if not.
        # Let's make the default api_chat_path more flexible.
        # If using the public HF API, base is /v1 and path is chat/completions.
        default_chat_path = 'chat/completions' if (configured_api_base_url and 'api-inference.huggingface.co/v1' in configured_api_base_url) else 'v1/chat/completions'
        chat_completions_path = hf_config.get('api_chat_path', default_chat_path).lstrip('/')

        if configured_api_base_url:
            # If api_base_url is configured, use it directly and append the chat_completions_path.
            # The model is expected to be in the payload.
            # If the endpoint needs the model_id in the path, configured_api_base_url should include it fully.
            api_url = f"{configured_api_base_url.rstrip('/')}/{chat_completions_path}"
            logging.info(f"HuggingFace: Using configured 'api_base_url' ('{configured_api_base_url}') and 'api_chat_path' ('{chat_completions_path}'). Target URL: {api_url}. Model is in payload.")
        else:
            # Fallback if no api_base_url is configured.
            # Use the public Hugging Face Inference API endpoint for OpenAI-like chat completions.
            default_hf_api_base = 'https://api-inference.huggingface.co/v1' # Base includes /v1
            default_chat_path_for_api_inference = 'chat/completions' # Path relative to /v1 base
            api_url = f"{default_hf_api_base.rstrip('/')}/{default_chat_path_for_api_inference}"
            logging.warning(
                f"HuggingFace: 'api_base_url' not configured. Defaulting to public Inference API endpoint: {api_url}. Model is in payload."
            )
    # --- End URL Construction ---

    final_temp = temp if temp is not None else _safe_cast(hf_config.get('temperature'), float, 0.7)
    # Ensure final_streaming is a boolean for the payload
    hf_config_streaming = hf_config.get('streaming', False)
    final_streaming_payload_val = streaming if streaming is not None else \
        (str(hf_config_streaming).lower() == 'true' if isinstance(hf_config_streaming, str) else bool(hf_config_streaming))


    # TGI uses max_new_tokens. OpenAI compatible layers might expect max_tokens.
    # If max_tokens is provided, prefer it. Otherwise, check hf_config for max_new_tokens or max_tokens
    final_max_val = max_tokens
    if final_max_val is None:
        final_max_val = _safe_cast(hf_config.get('max_tokens', hf_config.get('max_new_tokens')), int)


    api_messages = []
    # Handle system message: TGI usually wants it as the first message if no dedicated 'system' field in payload root
    # For OpenAI compatible /v1/chat/completions, system message is standard.
    if system_message:
        api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data) # input_data should be correctly formatted by caller

    payload: Dict[str, Any] = {
        "model": final_model_for_payload, # Model ID is crucial for endpoints that multiplex
        "messages": api_messages,
        "stream": final_streaming_payload_val, # Use the boolean value
    }

    if final_temp is not None: payload["temperature"] = final_temp
    if top_p is not None: payload["top_p"] = top_p
    if top_k is not None: payload["top_k"] = top_k
    if final_max_val is not None:
        # Use "max_tokens" for OpenAI compatibility, TGI might map this or use "max_new_tokens"
        # Sticking to "max_tokens" if the endpoint is /v1/chat/completions
        payload["max_tokens"] = final_max_val
    if seed is not None: payload["seed"] = seed
    if stop is not None: payload["stop_sequences"] = stop if isinstance(stop, list) else [stop] # TGI often uses stop_sequences
    if response_format is not None: payload["response_format"] = response_format # For OpenAI compatible JSON mode

    if num_return_sequences is not None and not final_streaming_payload_val : payload["n"] = num_return_sequences
    if user is not None: payload["user"] = user
    if tools is not None: payload["tools"] = tools
    if tool_choice is not None: payload["tool_choice"] = tool_choice
    if logit_bias is not None: payload["logit_bias"] = logit_bias
    if presence_penalty is not None: payload["presence_penalty"] = presence_penalty
    if frequency_penalty is not None: payload["frequency_penalty"] = frequency_penalty
    if logprobs is not None: payload["logprobs"] = logprobs
    if top_logprobs is not None and payload.get("logprobs"): payload["top_logprobs"] = top_logprobs


    # Remove None values from payload before sending, common practice
    payload = {k: v for k, v in payload.items() if v is not None}

    logging.debug(f"HuggingFace Final Payload (excluding messages, tools): {{ {', '.join(f'{k}: {v}' for k, v in payload.items() if k not in ['messages', 'tools'])} }}")
    if 'tools' in payload: logging.debug(f"HuggingFace Tools: {payload['tools']}")
    logging.debug(f"HuggingFace Headers: {headers}")

    timeout_seconds = float(hf_config.get('api_timeout', 120.0))
    # For streaming, timeout applies to initial connection and pauses between data.
    # Consider a tuple timeout (connect_timeout, read_timeout) for more control if needed.

    try:
        if final_streaming_payload_val: # Check the boolean intended for payload
            logging.debug(f"HuggingFace: Posting streaming request to {api_url}")
            # Session might not be strictly necessary for a single streaming POST, but good for potential keep-alive
            response = requests.post(api_url, headers=headers, json=payload, stream=True, timeout=timeout_seconds)
            response.raise_for_status()

            def stream_generator_huggingface():
                try:
                    for line_bytes in response.iter_lines():
                        if line_bytes:
                            decoded_line = line_bytes.decode('utf-8').strip()
                            if not decoded_line: continue # Skip empty keep-alive lines

                            # logging.debug(f"HF Stream raw line: {decoded_line}")
                            if decoded_line.startswith("data:"):
                                data_content = decoded_line[len("data:"):].strip()
                                if data_content == "[DONE]":
                                    logging.debug("HuggingFace stream received [DONE] marker.")
                                    break
                                try:
                                    chunk_json = json.loads(data_content)
                                    delta_content = chunk_json.get("choices", [{}])[0].get("delta", {}).get("content")
                                    if delta_content:
                                        yield delta_content
                                    # Consider if other parts of the chunk are needed, e.g., finish_reason in delta
                                    # For now, just yielding content as per OpenAI's typical text stream delta.
                                except json.JSONDecodeError:
                                    logging.warning(f"HuggingFace stream: JSON decode error for data: '{data_content}'")
                except requests.exceptions.ChunkedEncodingError as e_chunked:
                    logging.error(f"HuggingFace stream: ChunkedEncodingError during streaming: {e_chunked}")
                except Exception as e_stream:
                    logging.error(f"HuggingFace stream: Unexpected error during streaming: {e_stream}", exc_info=True)
                finally:
                    if response:
                        response.close() # Ensure response is closed
                    logging.debug("HuggingFace stream generator finished.")
            return stream_generator_huggingface()
        else: # Non-streaming
            logging.debug(f"HuggingFace: Posting non-streaming request to {api_url}")
            adapter = HTTPAdapter(max_retries=Retry(
                total=int(hf_config.get('api_retries', 3)),
                backoff_factor=float(hf_config.get('api_retry_delay', 1)),
                status_forcelist=[429, 500, 502, 503, 504],
                allowed_methods=["POST"] # Should be allowed_methods for Retry v0.9.2+ (urllib3)
                                         # or method_whitelist for older versions.
            ))
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)

            response = session.post(api_url, headers=headers, json=payload, timeout=timeout_seconds)
            response.raise_for_status()
            return response.json() # This should be an OpenAI compatible JSON response

    except requests.exceptions.HTTPError as e:
        status_code = getattr(e.response, 'status_code', 500)
        error_text = getattr(e.response, 'text', str(e))
        logging.error(f"HuggingFace API call failed to {api_url} with status {status_code}. Details: {error_text[:500]}", exc_info=False)
        if status_code == 401:
            raise ChatAuthenticationError(provider="huggingface", message=f"Authentication failed. Detail: {error_text[:200]}")
        elif status_code == 404: # Specifically handle 404 for URL/model issues
            raise ChatBadRequestError(provider="huggingface", message=f"Endpoint or Model not found (404) at {api_url}. Detail: {error_text[:200]}")
        elif status_code == 429:
            raise ChatRateLimitError(provider="huggingface", message=f"Rate limit exceeded. Detail: {error_text[:200]}")
        elif 400 <= status_code < 500: # Other 4xx
            raise ChatBadRequestError(provider="huggingface", message=f"Bad request (Status {status_code}) to {api_url}. Detail: {error_text[:200]}")
        else: # 5xx
            raise ChatProviderError(provider="huggingface", message=f"Server error (Status {status_code}) from {api_url}. Detail: {error_text[:200]}", status_code=status_code)
    except requests.exceptions.RequestException as e: # Covers DNS, Connection, Timeout errors
        logging.error(f"HuggingFace API request failed to {api_url} (network error): {e}", exc_info=True)
        raise ChatProviderError(provider="huggingface", message=f"Network error connecting to {api_url}: {e}", status_code=504) # 504 for timeout/gateway like
    except Exception as e:
        logging.error(f"HuggingFace API call to {api_url}: Unexpected error: {e}", exc_info=True)
        if not isinstance(e, ChatAPIError): # Avoid re-wrapping known chat errors
            raise ChatAPIError(provider="huggingface", message=f"Unexpected error in HuggingFace API call: {e}")
        else:
            raise # Re-raise if it's already a ChatAPIError subtype


def chat_with_mistral(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_message: Optional[str] = None,
        temp: Optional[float] = None,
        streaming: Optional[bool] = False,
        topp: Optional[float] = None,
        max_tokens: Optional[int] = None,
        random_seed: Optional[int] = None,
        top_k: Optional[int] = None,
        safe_prompt: Optional[bool] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None,
        response_format: Optional[Dict[str, str]] = None,
        custom_prompt_arg: Optional[str] = None
):
    loaded_config_data = load_and_log_configs()
    mistral_config = loaded_config_data.get('mistral_api', {})
    final_api_key = api_key or mistral_config.get('api_key')
    if not final_api_key:
        raise ChatConfigurationError(provider="mistral", message="Mistral API Key required.")

    # ... (logging key, model, temp, streaming, top_p setup) ...
    log_key = f"{final_api_key[:5]}...{final_api_key[-5:]}" if final_api_key and len(
        final_api_key) > 9 else "Key Provided"
    logging.debug(f"Mistral: Using API Key: {log_key}")
    current_model = model or mistral_config.get('model', 'mistral-large-latest')  # or mistral-small, mistral-medium
    current_temp = temp if temp is not None else float(
        mistral_config.get('temperature', 0.1))  # Mistral defaults to 0.7
    current_top_p = topp  # Mistral uses top_p
    current_streaming_cfg = mistral_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(
            current_streaming_cfg))

    current_max_tokens = max_tokens if max_tokens is not None else _safe_cast(mistral_config.get('max_tokens'), int)
    current_safe_prompt = safe_prompt if safe_prompt is not None else bool(mistral_config.get('safe_prompt', False))

    api_messages = []
    # Mistral expects system message as the first message with role: system if provided
    # However, their latest guidance often shows it as part of the first user message or specific instructions.
    # For OpenAI compatibility, if system_message is given, and not already in input_data, prepend it.
    has_system_in_input = any(msg.get("role") == "system" for msg in input_data)
    if system_message and not has_system_in_input:
        api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data)

    headers = {'Authorization': f'Bearer {final_api_key}', 'Content-Type': 'application/json',
               'Accept': 'application/json'}
    data = {"model": current_model, "messages": api_messages, "stream": current_streaming}

    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_max_tokens is not None: data["max_tokens"] = current_max_tokens
    if random_seed is not None: data["random_seed"] = random_seed  # Mistral uses random_seed
    if top_k is not None: data["top_k"] = top_k  # Mistral has top_k
    if current_safe_prompt is not None: data["safe_prompt"] = current_safe_prompt  # Mistral specific
    if tools is not None: data["tools"] = tools
    if tool_choice is not None: data["tool_choice"] = tool_choice  # "auto", "any", "none"
    if response_format is not None: data["response_format"] = response_format  # {"type": "json_object"}

    api_url = mistral_config.get('api_base_url', 'https://api.mistral.ai/v1').rstrip('/') + '/chat/completions'
    logging.debug(f"Mistral Request Payload (excluding messages): {{k: v for k, v in data.items() if k != 'messages'}}")

    try:
        if current_streaming:
            # ... (OpenAI-like streaming logic, use "Mistral" in logs) ...
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip(): yield line + "\n\n"
                    # ... (error handling for stream) ...
                    finally:
                        yield "data: [DONE]\n\n"
                        if response: response.close()

                return stream_generator()
        else:
            # ... (non-streaming, retry) ...
            adapter = HTTPAdapter(max_retries=Retry(total=int(mistral_config.get('api_retries', 3)),
                                                    backoff_factor=float(mistral_config.get('api_retry_delay', 1)),
                                                    status_forcelist=[429, 500, 502, 503, 504],
                                                    allowed_methods=["POST"]))
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            return response.json()
    except requests.exceptions.HTTPError as e:  # ... error handling ...
        raise
    except Exception as e:  # ... error handling ...
        raise ChatProviderError(provider="mistral", message=f"Unexpected error: {e}")


def chat_with_openrouter(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_message: Optional[str] = None,
        temp: Optional[float] = None,
        streaming: Optional[bool] = False,
        # OpenRouter specific names from your map
        top_p: Optional[float] = None,  # from generic topp
        top_k: Optional[int] = None,  # from generic topk
        min_p: Optional[float] = None,  # from generic minp (OpenRouter uses min_p not minp)
        max_tokens: Optional[int] = None,
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        response_format: Optional[Dict[str, str]] = None,
        n: Optional[int] = None,
        user: Optional[str] = None,  # from user_identifier
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        custom_prompt_arg: Optional[str] = None
):
    loaded_config_data = load_and_log_configs()
    openrouter_config = loaded_config_data.get('openrouter_api', {})
    # ... (api key, model, temp, streaming setup) ...
    final_api_key = api_key or openrouter_config.get('api_key')
    if not final_api_key: raise ChatConfigurationError(provider='openrouter', message="OpenRouter API Key required.")
    current_model = model or openrouter_config.get('model', 'mistralai/mistral-7b-instruct:free')
    # ... other param resolutions ...
    current_streaming_cfg = openrouter_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(
            current_streaming_cfg))

    api_messages = []
    if system_message: api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data)

    headers = {
        "Authorization": f"Bearer {final_api_key}", "Content-Type": "application/json",
        "HTTP-Referer": openrouter_config.get("site_url", "http://localhost"),  # OpenRouter specific
        "X-Title": openrouter_config.get("site_name", "TLDW-API"),  # OpenRouter specific
    }
    data = {"model": current_model, "messages": api_messages, "stream": current_streaming}
    # Add all other accepted parameters to data if they are not None
    if temp is not None: data["temperature"] = temp
    if top_p is not None: data["top_p"] = top_p
    if top_k is not None: data["top_k"] = top_k
    if min_p is not None: data["min_p"] = min_p  # OpenRouter uses min_p
    if max_tokens is not None: data["max_tokens"] = max_tokens
    if seed is not None: data["seed"] = seed
    if stop is not None: data["stop"] = stop
    if response_format is not None: data["response_format"] = response_format
    if n is not None: data["n"] = n
    if user is not None: data["user"] = user
    if tools is not None: data["tools"] = tools
    if tool_choice is not None: data["tool_choice"] = tool_choice
    if logit_bias is not None: data["logit_bias"] = logit_bias
    if presence_penalty is not None: data["presence_penalty"] = presence_penalty
    if frequency_penalty is not None: data["frequency_penalty"] = frequency_penalty
    if logprobs is not None: data["logprobs"] = logprobs
    if top_logprobs is not None and data.get("logprobs"): data["top_logprobs"] = top_logprobs

    api_url = openrouter_config.get('api_base_url', "https://openrouter.ai/api/v1").rstrip('/') + "/chat/completions"
    logging.debug(
        f"OpenRouter Request Payload (excluding messages): {{k: v for k, v in data.items() if k != 'messages'}}")

    try:
        if current_streaming:
            # ... (OpenAI-like streaming logic, ensure "OpenRouter" in logs) ...
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip(): yield line + "\n\n"
                    # ... (error handling for stream) ...
                    finally:
                        yield "data: [DONE]\n\n"
                        if response: response.close()

                return stream_generator()
        else:
            # ... (non-streaming logic) ...
            # ... (retry setup) ...
            adapter = HTTPAdapter(max_retries=Retry(total=int(openrouter_config.get('api_retries', 3)),
                                                    backoff_factor=float(openrouter_config.get('api_retry_delay', 1)),
                                                    status_forcelist=[429, 500, 502, 503, 504],
                                                    allowed_methods=["POST"]))
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            return response.json()  # OpenRouter usually returns OpenAI compatible JSON
    except requests.exceptions.HTTPError as e:  # ... error handling ...
        raise
    except Exception as e:  # ... error handling ...
        raise ChatProviderError(provider="openrouter", message=f"Unexpected error: {e}")

#
#
#######################################################################################################################